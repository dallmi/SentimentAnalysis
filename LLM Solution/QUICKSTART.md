# LLM Solution - Schnellstart

## âš¡ In 3 Minuten loslegen

### 1. Voraussetzungen

**Nur NumPy benÃ¶tigt:**
```bash
pip install numpy
```

Das war's! Keine weiteren Dependencies.

### 2. Erste Analyse

Erstelle eine neue Python-Datei `test.py`:

```python
from llm_sentiment_analyzer import LLMSentimentAnalyzer

# Initialisiere Analyzer
analyzer = LLMSentimentAnalyzer(use_bert=False)

# Analysiere Text
text = "This is an excellent article!"
result = analyzer.analyze(text)

print(f"Score: {result['score']}")
print(f"Category: {result['category']}")
```

AusfÃ¼hren:
```bash
cd "LLM Solution"
python test.py
```

### 3. Tests ausfÃ¼hren

```bash
cd "LLM Solution"
python test_llm_analyzer.py
```

Sollte alle 6 Tests bestehen! âœ“

## ğŸ“š Beispiele

```bash
# Alle Beispiele durchlaufen
python example_usage.py
```

## ğŸ¯ Schnelle Verwendung

### Einzelner Text

```python
from llm_sentiment_analyzer import LLMSentimentAnalyzer

analyzer = LLMSentimentAnalyzer(use_bert=False)
result = analyzer.analyze("Great article!")

print(result['score'])    # z.B. +0.85
print(result['category']) # z.B. 'positive'
```

### Mehrere Kommentare

```python
comments = [
    "Excellent article!",
    "Very helpful.",
    "Not clear.",
]

results = analyzer.analyze_batch(comments)

for comment, result in zip(comments, results):
    print(f"{comment}: {result['score']:+.2f}")
```

### Aggregierte Statistik

```python
aggregate = analyzer.get_aggregate_sentiment(comments)

print(f"Average: {aggregate['avg_score']}")
print(f"Positive: {aggregate['positive_ratio']:.1%}")
```

## ğŸŒ Multi-Language

UnterstÃ¼tzt automatisch:
- ğŸ‡¬ğŸ‡§ Englisch
- ğŸ‡©ğŸ‡ª Deutsch
- ğŸ‡«ğŸ‡· FranzÃ¶sisch
- ğŸ‡®ğŸ‡¹ Italienisch

```python
# Funktioniert mit allen Sprachen
texts = [
    "This is excellent!",      # EN
    "Das ist hervorragend!",   # DE
    "C'est excellent!",        # FR
    "Questo Ã¨ eccellente!",    # IT
]

for text in texts:
    result = analyzer.analyze(text)
    print(f"{text}: {result['score']:+.2f}")
```

## âš™ï¸ Modi

### Lexikon-Modus (empfohlen)
```python
# Schnell, effizient, gute Genauigkeit
analyzer = LLMSentimentAnalyzer(use_bert=False)
```

**Vorteile:**
- âš¡ Sehr schnell (~0.001s pro Text)
- ğŸ’¾ Wenig Speicher (~5 MB)
- âœ“ Gute Genauigkeit (~85%)

### BERT-Modus
```python
# HÃ¶here Genauigkeit, langsamer
analyzer = LLMSentimentAnalyzer(use_bert=True)
```

**Vorteile:**
- âœ“ HÃ¶here Genauigkeit (mit Training)
- ğŸ§  Besseres Kontext-VerstÃ¤ndnis

**Nachteil:**
- ğŸŒ Langsamer (~0.1s pro Text)
- ğŸ’¾ Mehr Speicher (~50 MB)
- âš ï¸ BenÃ¶tigt trainierte Gewichte fÃ¼r gute Ergebnisse

## ğŸ“Š DateigrÃ¶ÃŸe

```
LLM Solution/
â”œâ”€â”€ minimal_bert_tokenizer.py    9 KB
â”œâ”€â”€ minimal_bert_model.py       14 KB
â”œâ”€â”€ llm_sentiment_analyzer.py   14 KB
â”œâ”€â”€ test_llm_analyzer.py         8 KB
â”œâ”€â”€ example_usage.py             7 KB
â”œâ”€â”€ README.md                    9 KB
â”œâ”€â”€ QUICKSTART.md                3 KB
â””â”€â”€ __init__.py                  1 KB
                                â”€â”€â”€â”€â”€â”€
Total:                          ~65 KB
```

**Gesamt-Ordner: nur 112 KB!** âœ“

## ğŸ”§ Integration ins Hauptprojekt

### Methode 1: Direct Import

```python
import sys
sys.path.insert(0, 'LLM Solution')

from llm_sentiment_analyzer import LLMSentimentAnalyzer

analyzer = LLMSentimentAnalyzer(use_bert=False)
```

### Methode 2: In bestehenden Code integrieren

Bearbeite `src/sentiment_analyzer.py`:

```python
import sys
from pathlib import Path

# FÃ¼ge LLM Solution zum Path hinzu
llm_path = Path(__file__).parent.parent / "LLM Solution"
sys.path.insert(0, str(llm_path))

from llm_sentiment_analyzer import LLMSentimentAnalyzer

class SentimentAnalyzer:
    def __init__(self, use_llm=True):
        if use_llm:
            self.analyzer = LLMSentimentAnalyzer(use_bert=False)
        else:
            # Verwende original model
            from models.sentiment_model import LightweightSentimentAnalyzer
            self.analyzer = LightweightSentimentAnalyzer()

    def analyze_comment(self, comment: str):
        return self.analyzer.analyze(comment)
```

## ğŸ’¡ Tipps

1. **FÃ¼r Produktion**: Verwende Lexikon-Modus (schneller, effizienter)
2. **FÃ¼r Entwicklung**: Teste beide Modi
3. **Multi-Language**: Funktioniert automatisch, keine Konfiguration nÃ¶tig
4. **Batch-Verarbeitung**: Verwende `analyze_batch()` fÃ¼r viele Texte
5. **Custom Lexikon**: Erweitere das Lexikon fÃ¼r firmenspezifische Begriffe

## â“ HÃ¤ufige Fragen

**Q: Warum ist BERT neutral bei allen Texten?**
A: Das Model verwendet zufÃ¤llige Gewichte. FÃ¼r Produktion wÃ¼rden trainierte Gewichte verwendet.

**Q: Kann ich eigene WÃ¶rter hinzufÃ¼gen?**
A: Ja! Siehe README.md Abschnitt "Erweiterungen"

**Q: Welchen Modus soll ich verwenden?**
A: FÃ¼r die meisten FÃ¤lle: Lexikon-Modus (schnell + gut)

**Q: Funktioniert es offline?**
A: Ja! Keine Internet-Verbindung nÃ¶tig.

## ğŸ“– Weitere Dokumentation

- [README.md](README.md) - VollstÃ¤ndige Dokumentation
- [example_usage.py](example_usage.py) - AusfÃ¼hrliche Beispiele
- [test_llm_analyzer.py](test_llm_analyzer.py) - Test-Suite

## ğŸš€ NÃ¤chste Schritte

1. âœ“ Tests ausfÃ¼hren: `python test_llm_analyzer.py`
2. âœ“ Beispiele anschauen: `python example_usage.py`
3. âœ“ Ins Hauptprojekt integrieren
4. âœ“ Mit eigenen Daten testen

**Viel Erfolg!** ğŸ‰
